{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "collapsed_sections": [
        "KvNK_TySjfuU",
        "i1leI1sYjhsj",
        "DNxCKtWkjU1L"
      ],
      "mount_file_id": "1m98VBl0tpQZ2OEAtMpS-5YvO7xgwFWLA",
      "authorship_tag": "ABX9TyM+jps7zvQEIbP5v8EpaVQM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngsunjang/Class_DSU_OperatingSystem/blob/main/Skip_gram_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specs of Devices\n",
        "Colab primarily supports two kinds of GPU options\n",
        "*   A100 GPU -- not available as of now (Nov-17, 15:00PM)\n",
        "    - Allocation of high-performance GPUs is selective. It is known to be allocated according to priority.\n",
        "*   V100 GPU\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wh4JnOOXi9fk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. A100 GPU - Not available"
      ],
      "metadata": {
        "id": "KvNK_TySjfuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install GPUtil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmvwPT8ck8dE",
        "outputId": "5c85a0b1-5bcd-4076-cb28-fe8bb00ecedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting GPUtil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7395 sha256=672ad4dd821b4666897e8892ad07f910aa5f371ce3ef43607a5256b7208818ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil\n",
            "Successfully installed GPUtil-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import GPUtil"
      ],
      "metadata": {
        "id": "Y_kGGVpSkFzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. V100 GPU\n",
        "As below, the name of the V100 GPU memory is Tesla V100-SXM2-16GB.\n",
        "\n",
        "The GPU memory is 16384.0 MB (approx. 16 GB)."
      ],
      "metadata": {
        "id": "i1leI1sYjhsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of available GPUs\n",
        "gpus = GPUtil.getGPUs()\n",
        "\n",
        "# Print GPU information\n",
        "for i, gpu in enumerate(gpus):\n",
        "    print(f\"GPU {i + 1}:\")\n",
        "    print(f\"  Name: {gpu.name}\")\n",
        "    print(f\"  Driver: {gpu.driver}\")\n",
        "    print(f\"  GPU Memory: {gpu.memoryTotal} MB\")\n",
        "    print(f\"  GPU Memory Free: {gpu.memoryFree} MB\")\n",
        "    print(f\"  GPU Memory Used: {gpu.memoryUsed} MB\")\n",
        "    print(f\"  GPU Load: {gpu.load * 100}%\")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xbd_ZLwwi1Fs",
        "outputId": "8da30777-0832-498e-97aa-13d4dab121e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 1:\n",
            "  Name: Tesla V100-SXM2-16GB\n",
            "  Driver: 525.105.17\n",
            "  GPU Memory: 16384.0 MB\n",
            "  GPU Memory Free: 16150.0 MB\n",
            "  GPU Memory Used: 0.0 MB\n",
            "  GPU Load: 0.0%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive view of GPU specs\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIFTrgXLinU-",
        "outputId": "8be98c9a-6b4d-4630-deeb-42ad7c858d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 27 20:49:47 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    24W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Program"
      ],
      "metadata": {
        "id": "DNxCKtWkjU1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(10)\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import decomposition\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (10,8)\n",
        "import nltk\n",
        "#Import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "import time"
      ],
      "metadata": {
        "id": "UY-o4rk205jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "######################################\n",
        "# Passage split\n",
        "######################################\n",
        "import nltk\n",
        "\n",
        "# Download the punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQCbe4U21EnT",
        "outputId": "aca858c9-34b1-42ca-f73f-8d8ef691e0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juyFot090rp-"
      },
      "outputs": [],
      "source": [
        "# Specify the path to your text file\n",
        "# file_path = r\"C:\\Work\\CS\\1. Courseworks\\0. 2023 Fall\\CSC-718 Operating Systems & Parallel Programming\\Class Project\\Report 2\\OscarWild_TheDevotedFriend.txt\"\n",
        "file_path = \"/content/drive/MyDrive/Colab_Notebooks/Coursework/2023-Fall/OS/New_data.txt\"\n",
        "# file_path = \"./OscarWild_TheDevotedFriend_raw.txt\"\n",
        "# file_path = \"/content/GoogleNews.txt\"\n",
        "\n",
        "# Read the passage from the text file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    passage = file.read()\n",
        "\n",
        "# Use nltk to tokenize the text into sentences\n",
        "corpus = nltk.sent_tokenize(passage)\n",
        "######################################\n",
        "# print(corpus)# Record the end time\n",
        "\n",
        "# # Specify the path for the output .txt file\n",
        "# output_file_path = \"./Split_sentence.txt\"\n",
        "\n",
        "# # Write each element of the corpus list to the output file\n",
        "# with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "#     for sentence in corpus:\n",
        "#         output_file.write(sentence + '\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(corpus):\n",
        "    '''Creates a dictionary with all unique words in corpus with id'''\n",
        "    vocabulary = {}\n",
        "    i = 0\n",
        "    for s in corpus:\n",
        "        for w in s.split():\n",
        "            if w not in vocabulary:\n",
        "                vocabulary[w] = i\n",
        "                i+=1\n",
        "    return vocabulary\n",
        "\n",
        "def prepare_set(corpus, n_gram = 1):\n",
        "    '''Creates a dataset with Input column and Outputs columns for neighboring words.\n",
        "       The number of neighbors = n_gram*2'''\n",
        "    columns = ['Input'] + [f'Output{i+1}' for i in range(n_gram*2)]\n",
        "    result = pd.DataFrame(columns = columns)\n",
        "    for sentence in corpus:\n",
        "        for i,w in enumerate(sentence.split()):\n",
        "            inp = [w]\n",
        "            out = []\n",
        "            for n in range(1,n_gram+1):\n",
        "                # look back\n",
        "                if (i-n)>=0:\n",
        "                    out.append(sentence.split()[i-n])\n",
        "                else:\n",
        "                    out.append('<padding>')\n",
        "\n",
        "                # look forward\n",
        "                if (i+n)<len(sentence.split()):\n",
        "                    out.append(sentence.split()[i+n])\n",
        "                else:\n",
        "                    out.append('<padding>')\n",
        "            row = pd.DataFrame([inp+out], columns = columns)\n",
        "            result = result.append(row, ignore_index = True)\n",
        "    return result"
      ],
      "metadata": {
        "id": "vCUeHuh81IQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_set_ravel(corpus, n_gram = 1):\n",
        "    '''Creates a dataset with Input column and Output column for neighboring words.\n",
        "       The number of neighbors = n_gram*2'''\n",
        "    columns = ['Input', 'Output']\n",
        "    result = pd.DataFrame(columns = columns)\n",
        "    for sentence in corpus:\n",
        "        for i,w in enumerate(sentence.split()):\n",
        "            inp = w\n",
        "            for n in range(1,n_gram+1):\n",
        "                # look back\n",
        "                if (i-n)>=0:\n",
        "                    out = sentence.split()[i-n]\n",
        "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
        "                    result = result.append(row, ignore_index = True)\n",
        "\n",
        "                # look forward\n",
        "                if (i+n)<len(sentence.split()):\n",
        "                    out = sentence.split()[i+n]\n",
        "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
        "                    result = result.append(row, ignore_index = True)\n",
        "    return result"
      ],
      "metadata": {
        "id": "4OyIUdwl1J69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "3XBvUpzC1Pfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus):\n",
        "    result = []\n",
        "    for i in corpus:\n",
        "        out = nltk.word_tokenize(i)\n",
        "        out = [x.lower() for x in out]\n",
        "        out = [x for x in out if x not in stop_words]\n",
        "        result.append(\" \". join(out))\n",
        "    return result\n",
        "\n",
        "#########################\n",
        "# In paper, they used 300 dimensions and 5 context (n gram)\n",
        "#########################\n",
        "\n",
        "corpus = preprocess(corpus)\n",
        "# print(type(corpus), len(corpus), corpus)\n",
        "\n",
        "vocabulary = create_vocabulary(corpus)\n",
        "# print(vocabulary)\n",
        "\n",
        "train_emb = prepare_set(corpus, n_gram = 2)\n",
        "# print(train_emb.head())\n",
        "\n",
        "train_emb = prepare_set_ravel(corpus, n_gram = 2)\n",
        "# print(train_emb.head())\n",
        "\n",
        "train_emb.Input = train_emb.Input.map(vocabulary)\n",
        "train_emb.Output = train_emb.Output.map(vocabulary)\n",
        "# print(train_emb.head())\n",
        "\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "def get_input_tensor(tensor):\n",
        "    '''Transform 1D tensor of word indexes to one-hot encoded 2D tensor'''\n",
        "    size = [*tensor.shape][0]\n",
        "    inp = torch.zeros(size, vocab_size).scatter_(1, tensor.unsqueeze(1), 1.)\n",
        "    return Variable(inp).float()\n",
        "\n",
        "# embedding_dims = 5\n",
        "embedding_dims = 300\n",
        "# device = torch.device('cpu')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "initrange = 0.5 / embedding_dims\n",
        "W1 = Variable(torch.randn(vocab_size, embedding_dims, device=device).uniform_(-initrange, initrange).float(), requires_grad=True) # shape V*H\n",
        "W2 = Variable(torch.randn(embedding_dims, vocab_size, device=device).uniform_(-initrange, initrange).float(), requires_grad=True) #shape H*V\n",
        "print(f'W1 shape is: {W1.shape}, W2 shape is: {W2.shape}')\n",
        "\n",
        "# num_epochs = 2000\n",
        "num_epochs = 2000\n",
        "# learning_rate = 2e-1\n",
        "learning_rate = 10.0\n",
        "lr_decay = 0.99\n",
        "loss_hist = []\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rxqxj0C1Ltd",
        "outputId": "d80e53ca-c842-4d7f-9ddf-a1c58acc2909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 shape is: torch.Size([478, 300]), W2 shape is: torch.Size([300, 478])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epo in range(num_epochs):\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for x,y in zip(DataLoader(train_emb.Input.values, batch_size=train_emb.shape[0]), DataLoader(train_emb.Output.values, batch_size=train_emb.shape[0])):\n",
        "        # print('ground truth:', y, y.shape, type(y))\n",
        "        # one-hot encode input tensor\n",
        "        input_tensor = get_input_tensor(x).to(device) #shape N*V\n",
        "\n",
        "        # simple NN architecture\n",
        "        h = input_tensor.mm(W1.to(device)) # shape 1*H\n",
        "        # print(h.shape)\n",
        "        y_pred = h.mm(W2.to(device)) # shape 1*V\n",
        "        # print('prediction:', y_pred, y_pred.shape, type(y_pred))\n",
        "\n",
        "        # move target tensor to the same device\n",
        "        y = y.to(device)\n",
        "\n",
        "        # define loss func\n",
        "        loss_f = torch.nn.CrossEntropyLoss() # see details: https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "        #compute loss\n",
        "        loss = loss_f(y_pred, y)\n",
        "\n",
        "        # bakpropagation step\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights using gradient descent. For this step we just want to mutate\n",
        "        # the values of w1 and w2 in-place; we don't want to build up a computational\n",
        "        # graph for the update steps, so we use the torch.no_grad() context manager\n",
        "        # to prevent PyTorch from building a computational graph for the updates\n",
        "        with torch.no_grad():\n",
        "            # SGD optimization is implemented in PyTorch, but it's very easy to implement manually providing better understanding of process\n",
        "            W1 -= learning_rate*W1.grad.data\n",
        "            W2 -= learning_rate*W2.grad.data\n",
        "            # zero gradients for next step\n",
        "            W1.grad.data.zero_()\n",
        "            W2.grad.data.zero_()\n",
        "\n",
        "        # compute loss and accuracy\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        total_correct += (predicted == y).sum().item()\n",
        "        total_samples += y.size(0)\n",
        "\n",
        "    if epo%10 == 0:\n",
        "        learning_rate *= lr_decay\n",
        "    loss_hist.append(loss)\n",
        "    if epo%50 == 0:\n",
        "        accuracy = total_correct / total_samples\n",
        "        print(f'Epoch {epo}, loss = {loss}, accuracy = {accuracy}')\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(f\"Elapsed Time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1-wS9WX03Ng",
        "outputId": "744a25dc-df33-4fcd-a82f-cae6ae77f71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss = 6.169611930847168, accuracy = 0.00411814825333712\n",
            "Epoch 50, loss = 5.729260444641113, accuracy = 0.25901732462368643\n",
            "Epoch 100, loss = 5.5214948654174805, accuracy = 0.21073558648111332\n",
            "Epoch 150, loss = 374.6697692871094, accuracy = 0.13462084635046861\n",
            "Epoch 200, loss = 2.3877678954771907e+18, accuracy = 0.14456120420335133\n",
            "Epoch 250, loss = 9.402194832934897e+28, accuracy = 0.010934393638170975\n",
            "Epoch 300, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 350, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 400, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 450, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 500, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 550, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 600, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 650, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 700, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 750, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 800, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 850, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 900, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 950, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1000, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1050, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1100, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1150, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1200, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1250, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1300, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1350, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1400, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1450, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1500, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1550, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1600, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1650, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1700, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1750, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1800, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1850, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1900, loss = nan, accuracy = 0.005396194262993468\n",
            "Epoch 1950, loss = nan, accuracy = 0.005396194262993468\n",
            "Elapsed Time: 38.05966234207153 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hrxSALFw2Rhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
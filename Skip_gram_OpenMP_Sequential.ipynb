{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngsunjang/Class_DSU_OperatingSystem/blob/main/Skip_gram_OpenMP_Sequential.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Program"
      ],
      "metadata": {
        "id": "DNxCKtWkjU1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(10)\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import decomposition\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (10,8)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "torch.set_num_threads(1)  # Adjust this value in paranthesis for changing the number of threads"
      ],
      "metadata": {
        "id": "UY-o4rk205jt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######################################\n",
        "# Passage split\n",
        "######################################\n",
        "import nltk\n",
        "\n",
        "# Download the punkt tokenizer if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQCbe4U21EnT",
        "outputId": "a1672c24-5014-44f0-aeec-208ea1fa61f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "juyFot090rp-"
      },
      "outputs": [],
      "source": [
        "# Specify the path to your text file\n",
        "file_path = \"/content/New_data.txt\"\n",
        "\n",
        "# Read the passage from the text file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    passage = file.read()\n",
        "\n",
        "# Use nltk to tokenize the text into sentences\n",
        "corpus = nltk.sent_tokenize(passage)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vocabulary(corpus):\n",
        "    '''Creates a dictionary with all unique words in corpus with id'''\n",
        "    vocabulary = {}\n",
        "    i = 0\n",
        "    for s in corpus:\n",
        "        for w in s.split():\n",
        "            if w not in vocabulary:\n",
        "                vocabulary[w] = i\n",
        "                i+=1\n",
        "    return vocabulary\n",
        "\n",
        "def prepare_set(corpus, n_gram = 1):\n",
        "    '''Creates a dataset with Input column and Outputs columns for neighboring words.\n",
        "       The number of neighbors = n_gram*2'''\n",
        "    columns = ['Input'] + [f'Output{i+1}' for i in range(n_gram*2)]\n",
        "    result = pd.DataFrame(columns = columns)\n",
        "    for sentence in corpus:\n",
        "        for i,w in enumerate(sentence.split()):\n",
        "            inp = [w]\n",
        "            out = []\n",
        "            for n in range(1,n_gram+1):\n",
        "                # look back\n",
        "                if (i-n)>=0:\n",
        "                    out.append(sentence.split()[i-n])\n",
        "                else:\n",
        "                    out.append('<padding>')\n",
        "\n",
        "                # look forward\n",
        "                if (i+n)<len(sentence.split()):\n",
        "                    out.append(sentence.split()[i+n])\n",
        "                else:\n",
        "                    out.append('<padding>')\n",
        "            row = pd.DataFrame([inp+out], columns = columns)\n",
        "            result = result.append(row, ignore_index = True)\n",
        "    return result"
      ],
      "metadata": {
        "id": "vCUeHuh81IQ_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_set_ravel(corpus, n_gram = 1):\n",
        "    '''Creates a dataset with Input column and Output column for neighboring words.\n",
        "       The number of neighbors = n_gram*2'''\n",
        "    columns = ['Input', 'Output']\n",
        "    result = pd.DataFrame(columns = columns)\n",
        "    for sentence in corpus:\n",
        "        for i,w in enumerate(sentence.split()):\n",
        "            inp = w\n",
        "            for n in range(1,n_gram+1):\n",
        "                # look back\n",
        "                if (i-n)>=0:\n",
        "                    out = sentence.split()[i-n]\n",
        "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
        "                    result = result.append(row, ignore_index = True)\n",
        "\n",
        "                # look forward\n",
        "                if (i+n)<len(sentence.split()):\n",
        "                    out = sentence.split()[i+n]\n",
        "                    row = pd.DataFrame([[inp,out]], columns = columns)\n",
        "                    result = result.append(row, ignore_index = True)\n",
        "    return result"
      ],
      "metadata": {
        "id": "4OyIUdwl1J69"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "3XBvUpzC1Pfd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus):\n",
        "    result = []\n",
        "    for i in corpus:\n",
        "        out = nltk.word_tokenize(i)\n",
        "        out = [x.lower() for x in out]\n",
        "        out = [x for x in out if x not in stop_words]\n",
        "        result.append(\" \". join(out))\n",
        "    return result\n",
        "\n",
        "#########################\n",
        "# In paper, they used 300 dimensions and 5 context (n gram)\n",
        "#########################\n",
        "\n",
        "corpus = preprocess(corpus)\n",
        "vocabulary = create_vocabulary(corpus)\n",
        "train_emb = prepare_set(corpus, n_gram = 2)\n",
        "train_emb = prepare_set_ravel(corpus, n_gram = 2)\n",
        "train_emb.Input = train_emb.Input.map(vocabulary)\n",
        "train_emb.Output = train_emb.Output.map(vocabulary)\n",
        "\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "def get_input_tensor(tensor):\n",
        "    '''Transform 1D tensor of word indexes to one-hot encoded 2D tensor'''\n",
        "    size = [*tensor.shape][0]\n",
        "    inp = torch.zeros(size, vocab_size).scatter_(1, tensor.unsqueeze(1), 1.)\n",
        "    return Variable(inp).float()\n",
        "\n",
        "embedding_dims = 300\n",
        "device = torch.device('cpu')\n",
        "\n",
        "initrange = 0.5 / embedding_dims\n",
        "W1 = Variable(torch.randn(vocab_size, embedding_dims, device=device).uniform_(-initrange, initrange).float(), requires_grad=True) # shape V*H\n",
        "W2 = Variable(torch.randn(embedding_dims, vocab_size, device=device).uniform_(-initrange, initrange).float(), requires_grad=True) #shape H*V\n",
        "print(f'W1 shape is: {W1.shape}, W2 shape is: {W2.shape}')\n",
        "\n",
        "num_epochs = 2000\n",
        "learning_rate = 10.0\n",
        "lr_decay = 0.99\n",
        "loss_hist = []\n",
        "\n",
        "# Record the start time\n",
        "start_time = time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Rxqxj0C1Ltd",
        "outputId": "0e3d7a51-40ce-4535-d33c-7fd94196d656"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 shape is: torch.Size([478, 300]), W2 shape is: torch.Size([300, 478])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epo in range(num_epochs):\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for x,y in zip(DataLoader(train_emb.Input.values, batch_size=train_emb.shape[0]), DataLoader(train_emb.Output.values, batch_size=train_emb.shape[0])):\n",
        "        # one-hot encode input tensor\n",
        "        input_tensor = get_input_tensor(x).to(device) #shape N*V\n",
        "\n",
        "        # simple NN architecture\n",
        "        h = input_tensor.mm(W1.to(device)) # shape 1*H\n",
        "        y_pred = h.mm(W2.to(device)) # shape 1*V\n",
        "\n",
        "        # move target tensor to the same device\n",
        "        y = y.to(device)\n",
        "\n",
        "        # define loss func\n",
        "        loss_f = torch.nn.CrossEntropyLoss() # see details: https://pytorch.org/docs/stable/nn.html\n",
        "\n",
        "        #compute loss\n",
        "        loss = loss_f(y_pred, y)\n",
        "\n",
        "        # bakpropagation step\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights using gradient descent. For this step we just want to mutate\n",
        "        # the values of w1 and w2 in-place; we don't want to build up a computational\n",
        "        # graph for the update steps, so we use the torch.no_grad() context manager\n",
        "        # to prevent PyTorch from building a computational graph for the updates\n",
        "        with torch.no_grad():\n",
        "            # SGD optimization is implemented in PyTorch, but it's very easy to implement manually providing better understanding of process\n",
        "            W1 -= learning_rate*W1.grad.data\n",
        "            W2 -= learning_rate*W2.grad.data\n",
        "            # zero gradients for next step\n",
        "            W1.grad.data.zero_()\n",
        "            W2.grad.data.zero_()\n",
        "\n",
        "        # compute loss and accuracy\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        total_correct += (predicted == y).sum().item()\n",
        "        total_samples += y.size(0)\n",
        "\n",
        "    if epo%10 == 0:\n",
        "        learning_rate *= lr_decay\n",
        "    loss_hist.append(loss)\n",
        "    if epo%50 == 0:\n",
        "        accuracy = total_correct / total_samples\n",
        "        print(f'Epoch {epo}, loss = {loss}, accuracy = {accuracy}')\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# Print the elapsed time\n",
        "print(f\"Elapsed Time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1-wS9WX03Ng",
        "outputId": "80c4569d-85cd-4b6c-f96d-42405c90391c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss = 6.169610023498535, accuracy = 0.0008520306731042318\n",
            "Epoch 50, loss = 5.466715335845947, accuracy = 0.25901732462368643\n",
            "Epoch 100, loss = 5.735353946685791, accuracy = 0.2588753195115024\n",
            "Epoch 150, loss = 4.822391510009766, accuracy = 0.21144561204203352\n",
            "Epoch 200, loss = 6.614449977874756, accuracy = 0.15805168986083498\n",
            "Epoch 250, loss = 4.4044575691223145, accuracy = 0.17196819085487078\n",
            "Epoch 300, loss = 4.49868106842041, accuracy = 0.22337404146549275\n",
            "Epoch 350, loss = 4.3018412590026855, accuracy = 0.21854586765123543\n",
            "Epoch 400, loss = 4.342555522918701, accuracy = 0.2188298778756035\n",
            "Epoch 450, loss = 4.45860481262207, accuracy = 0.1621698381141721\n",
            "Epoch 500, loss = 4.304757118225098, accuracy = 0.21740982675376314\n",
            "Epoch 550, loss = 4.532461166381836, accuracy = 0.16259585345072422\n",
            "Epoch 600, loss = 4.564937591552734, accuracy = 0.16245384833854018\n",
            "Epoch 650, loss = 4.05201530456543, accuracy = 0.28358420903152515\n",
            "Epoch 700, loss = 3.998051643371582, accuracy = 0.2739278614030105\n",
            "Epoch 750, loss = 3.9597997665405273, accuracy = 0.26966770803748935\n",
            "Epoch 800, loss = 3.882383346557617, accuracy = 0.283016188582789\n",
            "Epoch 850, loss = 3.806243419647217, accuracy = 0.28486225504118146\n",
            "Epoch 900, loss = 3.794515609741211, accuracy = 0.28486225504118146\n",
            "Epoch 950, loss = 3.7530736923217773, accuracy = 0.28486225504118146\n",
            "Epoch 1000, loss = 3.721266269683838, accuracy = 0.28486225504118146\n",
            "Epoch 1050, loss = 3.6910083293914795, accuracy = 0.2850042601533655\n",
            "Epoch 1100, loss = 3.662505626678467, accuracy = 0.2850042601533655\n",
            "Epoch 1150, loss = 3.635668992996216, accuracy = 0.2850042601533655\n",
            "Epoch 1200, loss = 3.610379934310913, accuracy = 0.2850042601533655\n",
            "Epoch 1250, loss = 3.5865495204925537, accuracy = 0.2850042601533655\n",
            "Epoch 1300, loss = 3.5641298294067383, accuracy = 0.2850042601533655\n",
            "Epoch 1350, loss = 3.5431249141693115, accuracy = 0.2850042601533655\n",
            "Epoch 1400, loss = 3.5238184928894043, accuracy = 0.2850042601533655\n",
            "Epoch 1450, loss = 3.507249116897583, accuracy = 0.2850042601533655\n",
            "Epoch 1500, loss = 3.492576837539673, accuracy = 0.2850042601533655\n",
            "Epoch 1550, loss = 3.479478359222412, accuracy = 0.2850042601533655\n",
            "Epoch 1600, loss = 3.467397451400757, accuracy = 0.2850042601533655\n",
            "Epoch 1650, loss = 3.4562277793884277, accuracy = 0.2850042601533655\n",
            "Epoch 1700, loss = 3.445899724960327, accuracy = 0.2850042601533655\n",
            "Epoch 1750, loss = 3.4363484382629395, accuracy = 0.2850042601533655\n",
            "Epoch 1800, loss = 3.427513360977173, accuracy = 0.2850042601533655\n",
            "Epoch 1850, loss = 3.4193363189697266, accuracy = 0.2850042601533655\n",
            "Epoch 1900, loss = 3.411764144897461, accuracy = 0.2850042601533655\n",
            "Epoch 1950, loss = 3.4047465324401855, accuracy = 0.2850042601533655\n",
            "Elapsed Time: 476.83801007270813 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hrxSALFw2Rhl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}